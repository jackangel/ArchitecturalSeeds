
# Architectural Seeds for Emergent Abilities

**A collection of architectural and training techniques designed to encourage the early and reliable development of complex reasoning abilities in Large Language Models.**

---

## The Philosophy: From Emergence to Cultivation

Large Language Models (LLMs) have demonstrated a remarkable phenomenon known as **emergent abilities**. Skills like arithmetic, multi-step reasoning, and theory of mind are not explicitly programmed but appear spontaneously once a model surpasses a certain scale. This is often described as a "phase transition."

However, waiting for these abilities to emerge through brute-force scaling is computationally expensive, unpredictable, and gives us little control over the model's internal cognitive structures.

This repository explores **Architectural Seeding**, a paradigm shift from passive observation to active cultivation. The core idea is that just as a crystal forms around a "seed," we can introduce specific architectural priors and training objectives that act as seeds for desired cognitive patterns. These seeds make it more efficient for the model to learn complex abilities, encouraging them to form earlier, more robustly, and in smaller models.

We move from asking "what will emerge?" to "what can we cultivate?"

## Some Techniques

This repository explores several architectural seeds that can be integrated into various Transformer-based models.

### 1. The Discretized Manifold Block (Structured Representation)

*   **Analogy:** Creating a compressed "alphabet" of neural concepts. Instead of describing a complex idea with a lengthy, continuous vector, the model learns to represent it using a small set of discrete, pre-defined "letters" or "words" from a codebook.
*   **Core Idea:** The model enhances its internal representations by adding a discretized, quantized version of its own hidden state back to itself. This forces the model to represent information in a compressed, structured format using a learned codebook, which can improve robustness and abstract reasoning by creating a more organized internal state space.
*   **Architectural Implementation:** This technique is implemented in the **ZigguratLM** (https://github.com/jackangel/Experiment25_ZigguratLM) architecture not as a router for conditional computation, but as a representational bottleneck. Within each block, after the main attention and MLP sub-layers, the hidden state is processed by a **Residual Vector Quantizer (RVQ)**. This module approximates the continuous hidden state vector by finding the closest matching vectors from a series of codebooks (these codes are the "buckets"). This quantized vector, which serves as a compressed, structured representation, is then added back to the original hidden state. Every token passes through this process; there is no conditional routing.
*   **Pattern Seeded:** **Structured and Robust Representation.** By forcing the model to map its internal states to a discrete codebook, it is seeded with a pressure to develop a more organized and robust internal "language." This can lead to better generalization and the formation of more abstract, reusable concepts.

### 2. The Recurrent Scratchpad

*   **Analogy:** A private, mental workspace or a piece of scratch paper.
*   **Core Idea:** While the main model processes information in a parallel, feed-forward manner, the Recurrent Scratchpad provides a sequential, stateful mechanism *within* each layer. It allows the model to perform iterative, algorithmic computations on the representations generated by the parallel block.
*   **Architectural Implementation:** A small, recurrent unit (like a GRU or LSTM) is applied to the sequence of hidden states within a block. Its output is then projected back and added to the original representations, enriching them with sequentially processed information.
*   **Pattern Seeded:** **Algorithmic and multi-step reasoning.** The model is given a dedicated tool to "think step-by-step," making it easier to learn tasks that require intermediate calculations or state tracking (e.g., `(A+B)*C`).

### 3. The Internal Deliberator

*   **Analogy:** "Thinking twice" before giving an answer.
*   **Core Idea:** An initial, fast "System 1" guess is refined by one or more subsequent "System 2" deliberation steps. This is achieved by feeding the output of a layer (or a block of layers) back into itself for further processing before passing it on.
*   **Architectural Implementation:** A `for` loop within the `forward` pass that re-applies the final, most abstract layers of a block to their own output. This is a parameter-efficient way to deepen computation for complex inputs without increasing the model's size.
*   **Pattern Seeded:** **Iterative refinement and self-correction.** This encourages the model to develop circuits that can check and improve upon initial conclusions, leading to more robust and accurate reasoning.

### 4. The Confidence Circuit

*   **Analogy:** A model's "gut feeling" or intuition about its own uncertainty.
*   **Core Idea:** The model is trained on a multi-task objective. In addition to predicting the next token, a secondary "confidence head" is trained to predict the probability that the model's own primary prediction is correct.
*   **Architectural Implementation:** A small MLP is attached to the final hidden state. Its target is a binary label: `1.0` if the main `lm_head`'s `argmax` prediction matches the true token, and `0.0` otherwise. The loss from this head is added to the main cross-entropy loss.
*   **Pattern Seeded:** **Metacognition and calibrated uncertainty.** This forces the model to develop internal representations that correlate with its own competence, a foundational step towards more advanced abilities like strategic exploration or knowing when to ask for clarification.

## Other Promising Seeding Techniques (Future Research)

The principles of architectural seeding extend far beyond the techniques implemented here. Below are other promising avenues for research that follow the same philosophy.

#### 1. Causal Induction Heads
*   **Concept:** Standard induction heads learn to complete patterns (`A -> B, so A -> ?` becomes `B`). A *Causal Induction Head* would be trained with an auxiliary objective to differentiate between mere correlation and true causal intervention.
*   **Seeding Mechanism:** During training, expose the model to synthetic textual scenarios of interventions (`"Scientists added catalyst X, and the reaction sped up."`) and confounding (`"Ice cream sales and drowning incidents are correlated."`). An auxiliary loss would penalize the model for making incorrect causal inferences, forcing it to develop representations that track the flow of causality.

#### 2. Neuro-Symbolic Bottlenecks
*   **Concept:** Force information to pass through a low-dimensional, discrete, and potentially interpretable "symbolic" layer.
*   **Seeding Mechanism:** Integrate a Vector Quantizer (VQ) or a similar discretization module at a strategic point in the model (e.g., between major stages). This VQ's codes could be encouraged (via auxiliary losses) to represent logical operators, variables, or other symbolic concepts. This seeds a hybrid reasoning system that combines the pattern-matching strengths of neural networks with the formal logic of symbolic systems.

#### 3. Predictive World Models as an Auxiliary Task
*   **Concept:** Go beyond predicting the next token in language and learn to predict the next state of an abstract "world."
*   **Seeding Mechanism:** In addition to text, train the model on structured data representing simple state transitions (e.g., `state: {A:3, B:5}, action: "add 2 to A", next_state: {A:5, B:5}`). An auxiliary loss for predicting `next_state` would force the model to internalize the rules of the environment, seeding a foundational understanding of cause, effect, and object permanence.

## How to Use These Techniques

1.  **Identify the Target Ability:** Determine the cognitive pattern you wish to cultivate (e.g., algorithmic reasoning, efficient computation).
2.  **Select the Corresponding Seed:** Choose the architectural seed best suited for that ability (e.g., Recurrent Scratchpad, Discretized Manifold Block).
3.  **Modify Your Model:**
    *   Add the necessary modules (e.g., `nn.GRU`, vector quantizer) to your model's `__init__` method.
    *   Integrate the module into the `forward` pass at the appropriate location.
4.  **Adapt the Training Loop:** If the technique requires an auxiliary loss (like the Confidence Circuit), modify your training step to calculate and combine the losses.
